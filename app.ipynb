{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aba7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from flask import Flask, render_template, request\n",
    "# from flask_ngrok import run_with_ngrok\n",
    "import nltk\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e125b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat initialization\n",
    "model_lstm=load_model(\"model_lstm.h5\")\n",
    "model = load_model(\"chatbot_model.h5\")\n",
    "intents = json.loads(open(\"intents.json\").read())\n",
    "words = pickle.load(open(\"words.pkl\", \"rb\"))\n",
    "classes = pickle.load(open(\"classes.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6e2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "# run_with_ngrok(app) \n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad677d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @app.route(\"/get\", methods=[\"POST\"])\n",
    "# @app.route(\"/get\")\n",
    "# def chatbot_response():\n",
    "#     msg = request.args.get('msg')\n",
    "#     if (msg.startswith('my name is') or msg.startswith('I\\'m') or msg.startswith('I am')):\n",
    "#         name = msg[11:]\n",
    "#         ints = predict_class(msg, model)\n",
    "#         res1 = getResponse(ints, intents)\n",
    "#         res =res1.replace(\"{n}\",name)\n",
    "#     elif (msg.startswith('hi my name is') or msg.startswith('hi I\\'m')):\n",
    "#         name = msg[14:]\n",
    "#         ints = predict_class(msg, model)\n",
    "#         res1 = getResponse(ints, intents)\n",
    "#         res =res1.replace(\"{n}\",name)\n",
    "# #         print('elif 1 '+res)\n",
    "#     elif msg.startswith('description:'):\n",
    "#         msg = msg.partition('description:')[2]\n",
    "#         print(msg)\n",
    "#         res = predict_acc_level(msg,model_lstm)    \n",
    "# #         print('elif 2 '+res)\n",
    "#     else:\n",
    "#         ints = predict_class(msg, model)\n",
    "#         res = getResponse(ints, intents)\n",
    "# #         print('else 1 '+res)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42324e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route(\"/get\", methods=[\"POST\"])\n",
    "@app.route(\"/get\")\n",
    "def chatbot_response():\n",
    "    msg = request.args.get('msg')\n",
    "    if (msg.startswith('my name is') or msg.startswith('I\\'m') or msg.startswith('I am'.lower())):\n",
    "        name = msg.split()[-1]\n",
    "        ints = predict_class(msg, model)\n",
    "        res1 = getResponse(ints, intents)\n",
    "        res =res1.replace(\"{n}\",name)\n",
    "    elif (msg.startswith('hi my name is') or msg.startswith('hi I\\'m')):\n",
    "        name = msg.split()[-1]\n",
    "        ints = predict_class(msg, model)\n",
    "        res1 = getResponse(ints, intents)\n",
    "        res =res1.replace(\"{n}\",name)\n",
    "#         print('elif 1 '+res)\n",
    "    elif msg.startswith('description:'):\n",
    "        msg = msg.partition('description:')[2]\n",
    "        print(msg)\n",
    "        res = predict_acc_level(msg,model_lstm)    \n",
    "#         print('elif 2 '+res)\n",
    "    else:\n",
    "        ints = predict_class(msg, model)\n",
    "        res = getResponse(ints, intents)\n",
    "#         print('else 1 '+res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0f8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat functionalities\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# def get_bot_response():\n",
    "#     userText = request.args.get('msg')\n",
    "#     return str('Accident level unknown')\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag: %s\" % w)\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0][\"intent\"]\n",
    "    list_of_intents = intents_json[\"intents\"]\n",
    "    for i in list_of_intents:\n",
    "        if i[\"tag\"] == tag:\n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "#         inp = input()\n",
    "#         if inp.lower() == \"quit\":\n",
    "#             break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691d0c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Karthick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Karthick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Karthick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = ['covfefe']\n",
    "# nlp libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk import tokenize,stem\n",
    "\n",
    "# Keras pre-processing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb424e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "   # Initialize the object for Lemmatizer class\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # Set the stopwords to English\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Normalize the text in order deal with accented words and unicodes\n",
    "    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore').lower())\n",
    "\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    #word tokenization\n",
    "    words = nltk.word_tokenize(text.translate(remove_punct_dict))\n",
    "\n",
    "    # Consider the words which are not in stopwords of english and lemmatize them\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lems = [lemmatizer.lemmatize(i) for i in words if i not in stopwords]\n",
    "\n",
    "    return lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ef4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_acc_level(msg,model):\n",
    "    data = \" \".join(text_cleaning(msg))\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    #vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    #padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "    data = pad_sequences(data, maxlen=100, dtype='int32', value=0)\n",
    "    # print(data)\n",
    "    ypred = model.predict(data,batch_size=1,verbose = 2)[0]\n",
    "    print(ypred)\n",
    "    class_label=np.argmax(ypred)\n",
    "    if class_label == 0:\n",
    "        result ='AccidentLevel I'\n",
    "    elif class_label == 1:\n",
    "        result ='AccidentLevel II'\n",
    "    else:\n",
    "        result ='AccidentLevel III'\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e9da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 - 1s - 1s/epoch - 24ms/step\n",
      "[0.809407   0.08644677 0.10414631]\n",
      "AccidentLevel I\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AccidentLevel I'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_acc_level('approximately 1145 circumstance mechanic anthony group leader',model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a526818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:12] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:12] \"\u001b[33mGET /style.css HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:13] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:15] \"\u001b[37mGET /get?msg=hi HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:20] \"\u001b[37mGET /get?msg=my%20name%20is%20kar HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:30] \"\u001b[37mGET /get?msg=yes HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2021 18:35:44] \"\u001b[37mGET /get?msg=no HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32638e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
